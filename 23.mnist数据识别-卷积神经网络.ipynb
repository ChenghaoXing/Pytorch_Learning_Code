{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn,optim\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torchvision import datasets, transforms # 也是torch的工具，里面封装了更高级的功能\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练集\n",
    "train_dataset = datasets.MNIST(root='./', # 这里选择当前路径下载，意思就是说可以在当前路径MINST文件夹内找到数据，数据是封装好了的\n",
    "                            train = True, # 下载训练集\n",
    "                            transform = transforms.ToTensor(), #把数据变成基本的数据类型tensor\n",
    "                            download = True) # 直接从封装好接口上载入\n",
    "\n",
    "# 测试集\n",
    "test_dataset = datasets.MNIST(root='./', # 这里选择当前路径下载\n",
    "                            train = False, # 下载训练集\n",
    "                            transform = transforms.ToTensor(), #把数据变成基本的数据类型tensor\n",
    "                            download = True) # 直接从封装好接口上载入\n",
    "\n",
    "train_dataset # 这个东西已经被封装成了60000个点，使用Data_loader拿出来用就好\n",
    "len(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2ae379d1ca0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 批次大小，每次训练选择多少的数据\n",
    "batch_size = 64 \n",
    "\n",
    "# 装载训练集\n",
    "# dataloader相当于创建了一个数据生成器\n",
    "# 在这个里面数据就已经封装好了（train_loader和test_loader里面）\n",
    "train_loader = DataLoader(dataset = train_dataset,  # 数据的来源\n",
    "                          batch_size = batch_size,  # 批次的大小\n",
    "                          shuffle = True) # 数据是否进行打乱\n",
    "\n",
    "test_loader = DataLoader(dataset = test_dataset, # 来源于测试集\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle = True)\n",
    "\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n",
      "tensor([4, 4, 2, 1, 7, 6, 0, 8, 1, 6, 3, 4, 5, 6, 3, 0, 0, 8, 4, 2, 1, 1, 2, 5,\n",
      "        6, 4, 1, 0, 3, 0, 9, 3, 5, 7, 6, 0, 5, 6, 0, 8, 2, 0, 2, 0, 3, 9, 7, 0,\n",
      "        8, 9, 9, 5, 7, 0, 1, 0, 8, 9, 0, 5, 8, 0, 9, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据生成器使用方法——在循环中进行使用\n",
    "for i,data in enumerate(train_loader): #enumerate是为了每一批次的数据增加一个索引i\n",
    "    inputs, labels = data # data里面包含了要输入的量和输出的标签值\n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)\n",
    "    break\n",
    "print(labels)\n",
    "# 64个样本 1个通道（黑白） 28*28的像素\n",
    "# 标签同样是64个，标签值代表了是哪个数据\n",
    "len(train_loader) # 938*64 = 60032 刚好枚举完"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对以下网络的一些解释\n",
    "定义在网络结构里面的每一个背后都内含参数的\n",
    "全连接层会对不同的样本进行分别处理\n",
    "但是softmax是可以跨样本进行处理的（可能为了后续一些算法？）\n",
    "如果dim=0，则会对每一列进行归一化，不符合我们要求，所以要dim=1，完成行归一化\n",
    "\n",
    "# Dropout的相应用法\n",
    "Dropout一般在隐藏层中使用，编写代码的时候写在网络结构层编写\n",
    "在写网络结构时，可以使用封装代码\n",
    "非封装：self.fc1 = nn.Linear(784,500)\n",
    "封装版：self.layer1 = nn.Sequential(nn.Linear(784,500), nn.Dropout(p=0.5), nn.Tanh())\n",
    "然后调用的时候直接用layer1即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义网络结构 Pytorch，一个网络结构（初始化） 一个网络计算（得到结果）\n",
    "# 如果要用 Dropout，就要使用多层的网络结构，最后一层往往不卷积\n",
    "\n",
    "# 卷积操作函数 self.conv1 = nn.Sequential(nn.Conv2d(1,32,5,1,2), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "# 1是输入通道，32是输出通道，5是卷积核大小，1是步长，2是padding，2是池化核大小，2是步长\n",
    "# 如果输入的是（64，1，28，28），一次卷积后是（64，32，14，14）（包括池化），两次卷积后是（64，32，7，7）\n",
    "\n",
    "# 模型的大小一定要匹配！\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() # 是方法函数就需要用()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1,32,5,1,2), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(32,64,5,1,2), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.fc1 = nn.Sequential(nn.Linear(64*7*7,1000), nn.Dropout(p=0.5), nn.ReLU())\n",
    "        self.fc2 = nn.Sequential(nn.Linear(1000,10), nn.Softmax(dim=1)) # SoftMax dim=1 指的是对输出的每一行进行归一化（每一个样本）\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 输入的时候维度是[64,1,28,28], 全连接层只能处理2维数据，第一个维度是样本索引，第二个维度是样本内的\n",
    "        # ([64, 1, 28, 28])->(64, 784)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x) # ([64, 64, 7, 7])\n",
    "        x = x.reshape(x.size()[0],-1) # 这个函数是对x操作，要在x后调用  ([64, 64*7*7])\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义模型（对象化）\n",
    "model = Net() #对象化也是一个方法\n",
    "# 定义代价函数\n",
    "# 这是一个类，你需要实例化这个类！\n",
    "mse_loss = nn.CrossEntropyLoss() #交叉熵损失\n",
    "# 定义优化器\n",
    "# 要传入模型参数和学习率\n",
    "LR = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 进行模型的训练和测试方法——一边训练一边测试模型效果\n",
    "def train():\n",
    "    # 在dropout里面需要额外加上model.train()\n",
    "    model.train() # 代表是训练状态，此时模型的dropout是起作用的\n",
    "    for i,data in enumerate(train_loader): # 对迭代器进行分析\n",
    "        # 循环一次，就会获得一次数据的批次和标签，这很重要！\n",
    "        inputs, labels = data\n",
    "        # 获得模型预测结果, 这个结果是[64, 10]的一个矩阵，64是批次，10是概率值\n",
    "        out = model(inputs)\n",
    "        \n",
    "        # 用交叉熵损失，用交叉损失时，函数会内部自动转换成独热编码\n",
    "        # 其中，out是（64，10）数组，labels是64的一位数组\n",
    "        # 根据这个函数的用法，两者不需要对齐，只要保证第一个数都是minibatch值即可\n",
    "        loss = mse_loss(out, labels)\n",
    "        \n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 修改权值\n",
    "        optimizer.step()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    # 测试集准确率\n",
    "    correct = 0\n",
    "    for i,data in enumerate(test_loader): # 每一次都是在对一个训练好的模型进行评估，把所有的测试集跑了一边\n",
    "        inputs, labels = data\n",
    "        out = model(inputs)\n",
    "        # torch.max计算完后得到两个值，获得最大值和最大值所在的位置，dim=1代表对每一行进行操作\n",
    "        # 这里会对64行每一行都进行操作的\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        # 正确个数\n",
    "        correct += (predicted == labels).sum() # 这里面的true和false会一个一个对比\n",
    "    print('Test acc:{0}'.format(correct.item()/len(test_dataset)))\n",
    "\n",
    "    # 训练集准确率\n",
    "    correct = 0\n",
    "    for i,data in enumerate(train_loader): # 每一次都是在对一个训练好的模型进行评估，把所有的测试集跑了一边\n",
    "        inputs, labels = data\n",
    "        out = model(inputs)\n",
    "        # torch.max计算完后得到两个值，获得最大值和最大值所在的位置，dim=1代表对每一行进行操作\n",
    "        # 这里会对64行每一行都进行操作的\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        # 正确个数\n",
    "        correct += (predicted == labels).sum() # 这里面的true和false会一个一个对比\n",
    "    print('Train acc:{0}'.format(correct.item()/len(train_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout 使用场景\n",
    "并不一定让准确度变高，在模型比较复杂的情况下，应当使用，防止过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch:\u001b[39m\u001b[38;5;124m'\u001b[39m, epoch)\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     test()\n",
      "Cell \u001b[1;32mIn[33], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 计算梯度\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 修改权值\u001b[39;00m\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\18103\\anaconda3\\envs\\{AI_Training_Test}\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\18103\\anaconda3\\envs\\{AI_Training_Test}\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\18103\\anaconda3\\envs\\{AI_Training_Test}\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 循环运行\n",
    "# 获得了更快的收敛速度\n",
    "for epoch in range(20):\n",
    "    print('epoch:', epoch)\n",
    "    train()\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这里是一个常见的框架\n",
    "数据集的载入\n",
    "定义批次大小和分割数据集（接下去在循环中对这个数据进行训练）\n",
    "定义网络结构 和 网络运算 def __init__(self) 和 def forward(self, x)\n",
    "对象化模型 代价函数 优化器\n",
    "写网络如何训练测试——用两个模块封装好\n",
    "训练：输出结果 求损失 梯度归零 更新参数\n",
    "测试：输出结果\n",
    "\n",
    "一个大循环"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "{AI_Training_Test}",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
