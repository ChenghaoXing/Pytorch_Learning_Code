{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn,optim\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torchvision import datasets, transforms # 也是torch的工具，里面封装了更高级的功能\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练集\n",
    "train_dataset = datasets.MNIST(root='./', # 这里选择当前路径下载，意思就是说可以在当前路径MINST文件夹内找到数据，数据是封装好了的\n",
    "                            train = True, # 下载训练集\n",
    "                            transform = transforms.ToTensor(), #把数据变成基本的数据类型tensor\n",
    "                            download = True) # 直接从封装好接口上载入\n",
    "\n",
    "# 测试集\n",
    "test_dataset = datasets.MNIST(root='./', # 这里选择当前路径下载\n",
    "                            train = False, # 下载训练集\n",
    "                            transform = transforms.ToTensor(), #把数据变成基本的数据类型tensor\n",
    "                            download = True) # 直接从封装好接口上载入\n",
    "\n",
    "train_dataset # 这个东西已经被封装成了60000个点，使用Data_loader拿出来用就好\n",
    "len(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x233476d2940>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 批次大小，每次训练选择多少的数据\n",
    "batch_size = 64 \n",
    "\n",
    "# 装载训练集\n",
    "# dataloader相当于创建了一个数据生成器\n",
    "# 在这个里面数据就已经封装好了（train_loader和test_loader里面）\n",
    "train_loader = DataLoader(dataset = train_dataset,  # 数据的来源\n",
    "                          batch_size = batch_size,  # 批次的大小\n",
    "                          shuffle = True) # 数据是否进行打乱\n",
    "\n",
    "test_loader = DataLoader(dataset = test_dataset, # 来源于测试集\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle = True)\n",
    "\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n",
      "tensor([4, 8, 5, 8, 4, 1, 8, 7, 3, 3, 6, 5, 1, 1, 4, 6, 0, 8, 4, 8, 9, 8, 0, 7,\n",
      "        4, 5, 3, 4, 5, 1, 0, 0, 0, 3, 7, 2, 6, 9, 8, 6, 9, 2, 4, 0, 5, 0, 4, 0,\n",
      "        7, 7, 5, 5, 9, 0, 2, 7, 0, 0, 8, 7, 1, 7, 0, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据生成器使用方法——在循环中进行使用\n",
    "for i,data in enumerate(train_loader): #enumerate是为了每一批次的数据增加一个索引i\n",
    "    inputs, labels = data # data里面包含了要输入的量和输出的标签值\n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)\n",
    "    break\n",
    "print(labels)\n",
    "# 64个样本 1个通道（黑白） 28*28的像素\n",
    "# 标签同样是64个，标签值代表了是哪个数据\n",
    "len(train_loader) # 938*64 = 60032 刚好枚举完"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对以下网络的一些解释\n",
    "定义在网络结构里面的每一个背后都内含参数的\n",
    "全连接层会对不同的样本进行分别处理\n",
    "但是softmax是可以跨样本进行处理的（可能为了后续一些算法？）\n",
    "如果dim=0，则会对每一列进行归一化，不符合我们要求，所以要dim=1，完成行归一化\n",
    "\n",
    "# Dropout的相应用法\n",
    "Dropout一般在隐藏层中使用，编写代码的时候写在网络结构层编写\n",
    "在写网络结构时，可以使用封装代码\n",
    "非封装：self.fc1 = nn.Linear(784,500)\n",
    "封装版：self.layer1 = nn.Sequential(nn.Linear(784,500), nn.Dropout(p=0.5), nn.Tanh())\n",
    "然后调用的时候直接用layer1即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义网络结构 Pytorch，一个网络结构（初始化） 一个网络计算（得到结果）\n",
    "# 如果要用 Dropout，就要使用多层的网络结构\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() # 是方法函数就需要用()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(784,500), nn.Dropout(p=0), nn.Tanh())\n",
    "        self.layer2 = nn.Sequential(nn.Linear(500,300), nn.Dropout(p=0), nn.Tanh()) # 隐藏层是超参数\n",
    "        self.layer3 = nn.Sequential(nn.Linear(300,10), nn.Softmax(dim=1)) # 输出层不用Dropout，这些nn的方法第一个字母大写\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 输入的时候维度是[64,1,28,28], 全连接层只能处理2维数据，第一个维度是样本索引，第二个维度是样本内的\n",
    "        # ([64, 1, 28, 28])->(64, 784)\n",
    "        x = x.reshape(x.size()[0],-1) #-1代表自适应，变成64行，784列\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化\n",
    "正则化直接在optimizer里面进行调整即可！\n",
    "weight_decay 调整L2正则化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义模型（对象化）\n",
    "model = Net() #对象化也是一个方法\n",
    "# 定义代价函数\n",
    "# 这是一个类，你需要实例化这个类！\n",
    "mse_loss = nn.CrossEntropyLoss() #交叉熵损失\n",
    "# 定义优化器\n",
    "# 要传入模型参数和学习率\n",
    "# 在优化器里面可以加入正则化！！\n",
    "# weight_decay：设置L2正则化。\n",
    "LR = 0.5\n",
    "optimizer = optim.SGD(model.parameters(), LR, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 进行模型的训练和测试方法——一边训练一边测试模型效果\n",
    "def train():\n",
    "    # 在dropout里面需要额外加上model.train()\n",
    "    model.train() # 代表是训练状态，此时模型的dropout是起作用的\n",
    "    for i,data in enumerate(train_loader): # 对迭代器进行分析\n",
    "        # 循环一次，就会获得一次数据的批次和标签，这很重要！\n",
    "        inputs, labels = data\n",
    "        # 获得模型预测结果, 这个结果是[64, 10]的一个矩阵，64是批次，10是概率值\n",
    "        out = model(inputs)\n",
    "        \n",
    "        # 用交叉熵损失，用交叉损失时，函数会内部自动转换成独热编码\n",
    "        # 其中，out是（64，10）数组，labels是64的一位数组\n",
    "        # 根据这个函数的用法，两者不需要对齐，只要保证第一个数都是minibatch值即可\n",
    "        loss = mse_loss(out, labels)\n",
    "        \n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 修改权值\n",
    "        optimizer.step()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    # 测试集准确率\n",
    "    correct = 0\n",
    "    for i,data in enumerate(test_loader): # 每一次都是在对一个训练好的模型进行评估，把所有的测试集跑了一边\n",
    "        inputs, labels = data\n",
    "        out = model(inputs)\n",
    "        # torch.max计算完后得到两个值，获得最大值和最大值所在的位置，dim=1代表对每一行进行操作\n",
    "        # 这里会对64行每一行都进行操作的\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        # 正确个数\n",
    "        correct += (predicted == labels).sum() # 这里面的true和false会一个一个对比\n",
    "    print('Test acc:{0}'.format(correct.item()/len(test_dataset)))\n",
    "\n",
    "    # 训练集准确率\n",
    "    correct = 0\n",
    "    for i,data in enumerate(train_loader): # 每一次都是在对一个训练好的模型进行评估，把所有的测试集跑了一边\n",
    "        inputs, labels = data\n",
    "        out = model(inputs)\n",
    "        # torch.max计算完后得到两个值，获得最大值和最大值所在的位置，dim=1代表对每一行进行操作\n",
    "        # 这里会对64行每一行都进行操作的\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        # 正确个数\n",
    "        correct += (predicted == labels).sum() # 这里面的true和false会一个一个对比\n",
    "    print('Train acc:{0}'.format(correct.item()/len(train_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout 使用场景\n",
    "并不一定让准确度变高，在模型比较复杂的情况下，应当使用，防止过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch:\u001b[39m\u001b[38;5;124m'\u001b[39m, epoch)\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     test()\n",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m():\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# 在dropout里面需要额外加上model.train()\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# 代表是训练状态，此时模型的dropout是起作用的\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i,data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader): \u001b[38;5;66;03m# 对迭代器进行分析\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m# 循环一次，就会获得一次数据的批次和标签，这很重要！\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m# 获得模型预测结果, 这个结果是[64, 10]的一个矩阵，64是批次，10是概率值\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\18103\\anaconda3\\envs\\{AI_Training_Test}\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\18103\\anaconda3\\envs\\{AI_Training_Test}\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\18103\\anaconda3\\envs\\{AI_Training_Test}\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\18103\\anaconda3\\envs\\{AI_Training_Test}\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\18103\\anaconda3\\envs\\{AI_Training_Test}\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\18103\\anaconda3\\envs\\{AI_Training_Test}\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\18103\\anaconda3\\envs\\{AI_Training_Test}\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\18103\\anaconda3\\envs\\{AI_Training_Test}\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 循环运行\n",
    "# 获得了更快的收敛速度\n",
    "for epoch in range(20):\n",
    "    print('epoch:', epoch)\n",
    "    train()\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这里是一个常见的框架\n",
    "数据集的载入\n",
    "定义批次大小和分割数据集（接下去在循环中对这个数据进行训练）\n",
    "定义网络结构 和 网络运算 def __init__(self) 和 def forward(self, x)\n",
    "对象化模型 代价函数 优化器\n",
    "写网络如何训练测试——用两个模块封装好\n",
    "训练：输出结果 求损失 梯度归零 更新参数\n",
    "测试：输出结果\n",
    "\n",
    "一个大循环"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "{AI_Training_Test}",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
