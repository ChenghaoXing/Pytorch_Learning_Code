{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn,optim\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "from torchvision import datasets, transforms # 也是torch的工具，里面封装了更高级的功能\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练集\n",
    "train_dataset = datasets.MNIST(root='./', # 这里选择当前路径下载，意思就是说可以在当前路径MINST文件夹内找到数据，数据是封装好了的\n",
    "                            train = True, # 下载训练集\n",
    "                            transform = transforms.ToTensor(), #把数据变成基本的数据类型tensor\n",
    "                            download = True) # 直接从封装好接口上载入\n",
    "\n",
    "# 测试集\n",
    "test_dataset = datasets.MNIST(root='./', # 这里选择当前路径下载\n",
    "                            train = False, # 下载训练集\n",
    "                            transform = transforms.ToTensor(), #把数据变成基本的数据类型tensor\n",
    "                            download = True) # 直接从封装好接口上载入\n",
    "\n",
    "train_dataset # 这个东西已经被封装成了60000个点，使用Data_loader拿出来用就好\n",
    "len(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1788bd6e5b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 批次大小，每次训练选择多少的数据\n",
    "batch_size = 64 \n",
    "\n",
    "# 装载训练集\n",
    "# dataloader相当于创建了一个数据生成器\n",
    "# 在这个里面数据就已经封装好了（train_loader和test_loader里面）\n",
    "train_loader = DataLoader(dataset = train_dataset,  # 数据的来源\n",
    "                          batch_size = batch_size,  # 批次的大小\n",
    "                          shuffle = True) # 数据是否进行打乱\n",
    "\n",
    "test_loader = DataLoader(dataset = test_dataset, # 来源于测试集\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle = True)\n",
    "\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n",
      "tensor([5, 6, 6, 0, 5, 1, 5, 2, 4, 8, 2, 0, 7, 9, 8, 5, 8, 3, 8, 7, 9, 7, 7, 0,\n",
      "        4, 7, 2, 9, 5, 3, 2, 7, 8, 7, 1, 7, 2, 2, 0, 9, 4, 4, 8, 2, 0, 5, 9, 7,\n",
      "        1, 5, 7, 9, 8, 8, 3, 7, 8, 3, 0, 9, 8, 1, 1, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据生成器使用方法——在循环中进行使用\n",
    "for i,data in enumerate(train_loader): #enumerate是为了每一批次的数据增加一个索引i\n",
    "    inputs, labels = data # data里面包含了要输入的量和输出的标签值\n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)\n",
    "    break\n",
    "print(labels)\n",
    "# 64个样本 1个通道（黑白） 28*28的像素\n",
    "# 标签同样是64个，标签值代表了是哪个数据\n",
    "len(train_loader) # 938*64 = 60032 刚好枚举完"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对以下网络的一些解释\n",
    "定义在网络结构里面的每一个背后都内含参数的\n",
    "全连接层会对不同的样本进行分别处理\n",
    "但是softmax是可以跨样本进行处理的（可能为了后续一些算法？）\n",
    "如果dim=0，则会对每一列进行归一化，不符合我们要求，所以要dim=1，完成行归一化\n",
    "\n",
    "# Dropout的相应用法\n",
    "Dropout一般在隐藏层中使用，编写代码的时候写在网络结构层编写\n",
    "在写网络结构时，可以使用封装代码\n",
    "非封装：self.fc1 = nn.Linear(784,500)\n",
    "封装版：self.layer1 = nn.Sequential(nn.Linear(784,500), nn.Dropout(p=0.5), nn.Tanh())\n",
    "然后调用的时候直接用layer1即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义网络结构 Pytorch，一个网络结构（初始化） 一个网络计算（得到结果）\n",
    "# 使用LSTM模型来进行图像的处理，图片本身是(64,1,28,28)，要把图片按行的顺序喂进去，喂28次\n",
    "# LSTM各个参数含义\n",
    "# input_size 每一个样本输入进去的序列长度（比如图片28行，则为28）\n",
    "# hidden_size LSTM每一层的模块数，这里取64，则序列里每一个xn输出的会有64个hn\n",
    "# num_layers 层数，一般1层即可\n",
    "# batch_first 默认的input为（seq_len, batch, feature），True后为（batch, seq_len, feature），是三维数据\n",
    "# 模型的大小一定要匹配！\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size = 28,\n",
    "            hidden_size = 64,\n",
    "            num_layers = 1,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.out = torch.nn.Linear(in_features=64, out_features=10) # 我只取sequence最后一个对应的输出，进行全连接\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 输入的时候维度是（64,1,28,28）, 现在我要变成三维数据来处理\n",
    "        # output：[batch, seq_len, hidden_size]包含每个序列输出的结果\n",
    "        # h_n：[num_layers, batch, hidden_size]只包含最后一个序列的输出结果\n",
    "        # c_n：[num_layers, batch, hidden_size]\n",
    "        x = x.reshape(-1, 28, 28) # 这里就变成了（64，28，28），可以输入LSTM中\n",
    "        output, (h_n, c_n) = self.lstm(x)\n",
    "        output_in_last_timestep = h_n[-1,:,:]\n",
    "        x = self.out(output_in_last_timestep)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义模型（对象化）\n",
    "model = LSTM() #对象化也是一个方法\n",
    "# 定义代价函数\n",
    "# 这是一个类，你需要实例化这个类！\n",
    "mse_loss = nn.CrossEntropyLoss() #交叉熵损失\n",
    "# 定义优化器\n",
    "# 要传入模型参数和学习率\n",
    "LR = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 进行模型的训练和测试方法——一边训练一边测试模型效果\n",
    "def train():\n",
    "    # 在dropout里面需要额外加上model.train()\n",
    "    model.train() # 代表是训练状态，此时模型的dropout是起作用的\n",
    "    for i,data in enumerate(train_loader): # 对迭代器进行分析\n",
    "        # 循环一次，就会获得一次数据的批次和标签，这很重要！\n",
    "        inputs, labels = data\n",
    "        # 获得模型预测结果, 这个结果是[64, 10]的一个矩阵，64是批次，10是概率值\n",
    "        out = model(inputs)\n",
    "        \n",
    "        # 用交叉熵损失，用交叉损失时，函数会内部自动转换成独热编码\n",
    "        # 其中，out是（64，10）数组，labels是64的一位数组\n",
    "        # 根据这个函数的用法，两者不需要对齐，只要保证第一个数都是minibatch值即可\n",
    "        loss = mse_loss(out, labels)\n",
    "        \n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 修改权值\n",
    "        optimizer.step()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    # 测试集准确率\n",
    "    correct = 0\n",
    "    for i,data in enumerate(test_loader): # 每一次都是在对一个训练好的模型进行评估，把所有的测试集跑了一边\n",
    "        inputs, labels = data\n",
    "        out = model(inputs)\n",
    "        # torch.max计算完后得到两个值，获得最大值和最大值所在的位置，dim=1代表对每一行进行操作\n",
    "        # 这里会对64行每一行都进行操作的\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        # 正确个数\n",
    "        correct += (predicted == labels).sum() # 这里面的true和false会一个一个对比\n",
    "    print('Test acc:{0}'.format(correct.item()/len(test_dataset)))\n",
    "\n",
    "    # 训练集准确率\n",
    "    correct = 0\n",
    "    for i,data in enumerate(train_loader): # 每一次都是在对一个训练好的模型进行评估，把所有的测试集跑了一边\n",
    "        inputs, labels = data\n",
    "        out = model(inputs)\n",
    "        # torch.max计算完后得到两个值，获得最大值和最大值所在的位置，dim=1代表对每一行进行操作\n",
    "        # 这里会对64行每一行都进行操作的\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        # 正确个数\n",
    "        correct += (predicted == labels).sum() # 这里面的true和false会一个一个对比\n",
    "    print('Train acc:{0}'.format(correct.item()/len(train_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout 使用场景\n",
    "并不一定让准确度变高，在模型比较复杂的情况下，应当使用，防止过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch:\u001b[39m\u001b[38;5;124m'\u001b[39m, epoch)\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     test()\n",
      "Cell \u001b[1;32mIn[20], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m():\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# 在dropout里面需要额外加上model.train()\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# 代表是训练状态，此时模型的dropout是起作用的\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i,data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader): \u001b[38;5;66;03m# 对迭代器进行分析\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m# 循环一次，就会获得一次数据的批次和标签，这很重要！\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m# 获得模型预测结果, 这个结果是[64, 10]的一个矩阵，64是批次，10是概率值\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\18103\\anaconda3\\envs\\{AI_Training_Test}\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\18103\\anaconda3\\envs\\{AI_Training_Test}\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\18103\\anaconda3\\envs\\{AI_Training_Test}\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\18103\\anaconda3\\envs\\{AI_Training_Test}\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\18103\\anaconda3\\envs\\{AI_Training_Test}\\lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\18103\\anaconda3\\envs\\{AI_Training_Test}\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\18103\\anaconda3\\envs\\{AI_Training_Test}\\lib\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 循环运行\n",
    "# 获得了更快的收敛速度\n",
    "for epoch in range(20):\n",
    "    print('epoch:', epoch)\n",
    "    train()\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这里是一个常见的框架\n",
    "数据集的载入\n",
    "定义批次大小和分割数据集（接下去在循环中对这个数据进行训练）\n",
    "定义网络结构 和 网络运算 def __init__(self) 和 def forward(self, x)\n",
    "对象化模型 代价函数 优化器\n",
    "写网络如何训练测试——用两个模块封装好\n",
    "训练：输出结果 求损失 梯度归零 更新参数\n",
    "测试：输出结果\n",
    "\n",
    "一个大循环"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "{AI_Training_Test}",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
